<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Shyamal Dharia</title>
    <script src="javascript.js"></script>
    <link rel="stylesheet" href="./styles/nav.css">
    <link rel="stylesheet" href="./styles/description_container.css">
    <link rel="stylesheet" href="./styles/general.css">
    <link rel="stylesheet" href="./styles/profile_picture.css">
    <link rel="stylesheet" href="./styles/project.css">
    <link rel="stylesheet" href="./styles/publication.css">
    <link rel="stylesheet" href="./styles/skills-container.css">
    <style>

    </style>
</head>
<body>
<div class="full-container">
    <div class="description-container">
        <div class="Image">
            <div class="profile-picture">
                <img src="./assets/profile-picture/img.jpg" alt="my image" height="224" width="244">
            </div>
            <div>
                <p class="links">
                    <a class="github-logo" href="https://github.com/Shyamal-Dharia" target="_blank">
                        <img src="./assets/logos/github.png" alt="GitHub" height="30" width="30">
                    </a>
                    <a class="linkedin-logo" href="https://www.linkedin.com/in/shyamaldharia/" target="_blank">
                        <img src="./assets/logos/linkedin.png" alt="LinkedIn" height="30" width="30">
                    </a>
                    <a class="email-logo" href="mailto:shyamaldharia2@gmail.com" target="_blank">
                        <img src="./assets/logos/cemail.svg" alt="email" height="30" width="30">
                    </a>
                </p>
            </div>
        </div>
        <div class="description">
            <h2>Shyamal Dharia</h2>
            <p>
                I'm a Senior Researcher at <a href="https://www.uwinnipeg.ca/" target="_blank">The University of Winnipeg</a>,
                Supervised by <a href="https://sites.google.com/view/camilovalderrama/" target="_blank">Dr. Camilo Valderrama</a>, 
                <a href="https://www.acs.uwinnipeg.ca/liu-qi/" target="_blank">Dr. Qian Liu</a>, and
                <a href="https://www.uwinnipeg.ca/psychology/faculty-staff/stephen-smith.html#" target="_blank">Dr. Stephen Smith</a> . Currently working on a <strong>healthcare research</strong>.
            </p>
            <p>
                I completed my MSc in Applied Computer Science from the
                <a href="https://www.uwinnipeg.ca/" target="_blank">University of Winnipeg</a>. Before that, I obtained my B. Eng. degree in Electronics &amp; Communication Engineering at
                <a href="https://www.gtu.ac.in/" target="_blank">Gujarat Technological University</a>.
            </p>
            

            
            <p>
                My research interests include <strong>Machine Learning</strong>, <strong>Deep Learning</strong>, and <strong>Computer Vision</strong>. I am particularly interested in the application of these technologies in the field of healthcare.
            <p>
                <a href="./documents/Shyamal CV 2023.pdf" download="Shyamal-CV" class="cv-button" target="_blank">Download CV</a>
            </p>
        </div>
        
    </div>





    <div class="publication-container">
        <div class="publication-heading"><h2>Publications</h2></div>
        <div class="publication">
            <div class="pub-image">
                <img src="./assets/publications/Dual-Transformer Cross-Attention Framework for Alzheimer's Disease Detection Via dPTE-Guided EEG Channel Selection and Multi-Modal Integration.png">
            </div>
            <div class="pub-details">
                <p class="pub-title">
                    <a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5204707" target="_blank">
                        <strong>Dual-Transformer Cross-Attention Framework for Alzheimer's Disease Detection Via dPTE-Guided EEG Channel Selection and Multi-Modal Integration (Under Review)</strong>
                    </a>
                </p>
                <p class="pub-citation">
                    Dharia, Shyamal Yogeshchandra, Liu, Qian, Smith, Stephen, Valderrama Cuadros, Camilo Ernesto, "Dual-Transformer Cross-Attention Framework for Alzheimer's Disease Detection Via dPTE-Guided EEG Channel Selection and Multi-Modal Integration," Available at SSRN: <a href="https://ssrn.com/abstract=5204707" target="_blank">https://ssrn.com/abstract=5204707</a> or <a href="http://dx.doi.org/10.2139/ssrn.5204707" target="_blank">http://dx.doi.org/10.2139/ssrn.5204707</a>.
                </p>
            </div>
        </div>
        <div class="publication">
            <div class="pub-image">
                <img src="./assets/publications/Leveraging Machine Learning and Threshold-Free Cluster Enchancement to Unravel Perecption of Emotion and Implied Movement.png">
            </div>
            <div class="pub-details">
                <p class="pub-title">
                    <a href="https://ieeexplore.ieee.org/document/10913664" target="_blank">
                        <strong>Leveraging Machine Learning and Threshold-Free Cluster Enhancement to Unravel Perception of Emotion and Implied Movement</strong>
                    </a>
                </p>
                <p class="pub-citation">
                    S. Y. Dharia, M. Hojjati, S. G. Camorlinga, S. D. Smith and A. S. Desroches, "<a href="https://ieeexplore.ieee.org/document/10913664" target="_blank">Leveraging Machine Learning and Threshold-Free Cluster Enhancement to Unravel Perception of Emotion and Implied Movement</a>," 2024 IEEE EMBS International Conference on Biomedical and Health Informatics (BHI), Houston, TX, USA, 2024, pp. 1-8, doi: 10.1109/BHI62660.2024.10913664.
                </p>
            </div>
        </div>

        <div class="publication">
            <div class="pub-image">
                <img src="./assets/publications/dataset_independent_channel_selection_2.png">
            </div>
            <div class="pub-details">
                <p class="pub-title">
                    <a href="https://ieeexplore.ieee.org/abstract/document/10782444" target="_blank">
                        <strong>Dataset-Independent EEG Channel Selection for Emotion Recognition
                        </strong>
                    </a>
                </p>
                <p class="pub-citation">
                    S. Y. Dharia, S. G. Camorlinga, C. E. Valderrama and M. Hojjati, "<a href="https://ieeexplore.ieee.org/abstract/document/10782444" target="_blank">Dataset-Independent EEG Channel Selection for Emotion Recognition</a>," 2024 46th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC), Orlando, FL, USA, 2024, pp. 1-4, doi: 10.1109/EMBC53108.2024.10782444.                </p>
            </div>
        </div>
    </div>
        <div class="publication">
            <div class="pub-image">
                <img src="./assets/publications/Multimodal Deep Learning Model for Subject-Independent EEG-based Emotion Recognition.png">
            </div>
            <div class="pub-details">
                <p class="pub-title">
                    <a href="https://ieeexplore.ieee.org/document/10289007" target="_blank">
                        <strong>Multimodal Deep Learning Model for Subject-Independent EEG-based Emotion Recognition</strong>
                    </a>
                </p>
                <p class="pub-citation">
                    S. Y. Dharia, C. E. Valderrama and S. G. Camorlinga, "<a href="https://ieeexplore.ieee.org/document/10289007" target="_blank">Multimodal Deep Learning Model for Subject-Independent EEG-based Emotion Recognition</a>," 2023 IEEE Canadian Conference on Electrical and Computer Engineering (CCECE), Regina, SK, Canada, 2023, pp. 105-110, doi: 10.1109/CCECE58730.2023.10289007.
                </p>
            </div>
    </div>


    <div class="project-container">
        <div class="project-heading"><h2>Projects</h2></div>
        <div class="project">
            <div class="project-image">
                <img src="./assets/projects/vit_architecture.jpg">
                <div class="citation-text">
                    <p>ViT architecture. Taken from the <a href="https://arxiv.org/abs/2010.11929" target="_blank">original paper</a>.</p>
                </div>
            </div>
            <div class="project-details">
                <p class="project-title">
                    <a href="https://github.com/Shyamal-Dharia/vision-transformers/tree/main" target="_blank">
                        <strong>Vision Transformer (ViT) Models for Image and Time Series Classification</strong>
                    </a>
                </p>
                <p class="project-description">
                    ViTs from scratch in PyTorch and fine-tuned pre-trained ViT model using ImageNet weights on the Caltech101 dataset, achieving a remarkable 94% test accuracy.
                </p>
                <p class="project-link">
                    <a class="github-logo" href="https://github.com/Shyamal-Dharia/vision-transformers/tree/main" target="_blank">
                        <img src="./assets/logos/github.png" alt="GitHub" height="30" width="30">
                    </a>
                </p>
            </div>
        </div>

        <div class="project">
            <div class="project-image">
                <img src="./assets/projects/U-Net.jpg">
                <div class="citation-text">
                    <p>U-Net architecture. Taken from the <a href="https://arxiv.org/abs/1505.04597" target="_blank">original paper</a>.</p>
                </div>
            </div>
            <div class="project-details">
                <p class="project-title">
                    <a href="https://github.com/Shyamal-Dharia/U-Net-for-Image-Segmentation" target="_blank">
                        <strong>U-Net for Human and Aerial image Segmentation</strong>
                    </a>
                </p>
                <p class="project-description">
                    Utilized U-Net for human and aerial image segmentation in PyTorch. Enhanced performance with a pre-trained U-Net with ImageNet weights for accurate segmentation on both Human Segmentation and Massachusetts Roads Datasets.
                </p>
                <p class="project-link">
                    <a class="github-logo" href="https://github.com/Shyamal-Dharia/U-Net-for-Image-Segmentation" target="_blank">
                        <img src="./assets/logos/github.png" alt="GitHub" height="30" width="30">
                    </a>
                </p>
            </div>
        </div>
    </div>
</div>
</body>
</html>
